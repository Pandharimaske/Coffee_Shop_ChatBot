{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pandhari/Desktop/Coffee_Shop_ChatBot/Python_Backend/Backend/experiments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pandhari/Desktop/Coffee_Shop_ChatBot/Python_Backend/Backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pandhari/Desktop/Coffee_Shop_ChatBot/Python_Backend\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Backend.graph.coffee_shop_graph import build_coffee_shop_graph\n",
    "build = build_coffee_shop_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image , display\n",
    "# display(Image(build.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Backend.utils.memory_manager import get_user_memory\n",
    "# from Backend.utils.summary_memory import get_summary , get_messages\n",
    "# user_id = 15\n",
    "# state = {\n",
    "#             \"user_memory\": None,\n",
    "#             \"messages\" : [] ,\n",
    "#             \"chat_summary\": \"\",\n",
    "#             \"user_input\": \"\",\n",
    "#             \"response_message\": None,\n",
    "#             \"decision\": None,\n",
    "#             \"target_agent\": None,\n",
    "#             \"order\": [],\n",
    "#             \"final_price\": None,\n",
    "#             \"memory_node\": False,\n",
    "#         }\n",
    "\n",
    "# state[\"user_input\"] = \"Hlo this is aski\"\n",
    "        \n",
    "# state[\"user_memory\"] = get_user_memory(user_id)\n",
    "# state[\"chat_summary\"] = get_summary(id=user_id)\n",
    "# state[\"messages\"] = get_messages(id = user_id)\n",
    "\n",
    "# config = {\n",
    "#         \"configurable\": {\n",
    "#         \"user_id\": user_id\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build.stream(state , config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from App.chatbot import get_bot_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting get_user_memory\n",
      "Done get_user_memory\n",
      "Starting get_summary\n",
      "Done get_summary\n",
      "Starting get_messages\n",
      "Done get_messages\n",
      "Starting get_order\n",
      "Started Get Order\n",
      "Done get_order\n",
      "updates=[OrderUpdateItem(name='Cappuccino', set_quantity=3, delta_quantity=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ai/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Order\n",
      "Started Serializing Order\n",
      "{'response': \"I've updated your order to 3 Cappuccinos, which comes out to be ₹13.50. We'll get those made fresh for you right away. Would you like to have them to stay or to go?\", 'state': {'user_memory': UserMemory(name=None, likes=[], dislikes=[], allergies=[], last_order=None, feedback=[], location=None), 'messages': [HumanMessage(content='make the Cappuccino 3', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I've updated your order to 3 Cappuccinos, which comes out to be ₹13.50. We'll get those made fresh for you right away. Would you like to have them to stay or to go?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='order a Cappuccino for me', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'd be happy to get that Cappuccino ready for you. Just to confirm, I have you down for a single Cappuccino, which comes out to be $4.5. Shall I go ahead and place the order for you?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='tell me about your Cappuccino', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello. I'd be happy to tell you about our Cappuccino. It's a popular item on our menu, priced at $4.5, and has a fantastic rating of 4.7. Our Cappuccino is made with a rich shot of espresso, steamed milk, and a layer of creamy foam on top. We take great care in crafting each drink to ensure the perfect balance of flavors. Would you like to know more about it, such as the type of milk we use or if you can customize it to your taste? Or is there something else I can help you with today?\", additional_kwargs={}, response_metadata={})], 'chat_summary': None, 'user_input': 'make the Cappuccino 3', 'response_message': \"I've updated your order to 3 Cappuccinos, which comes out to be ₹13.50. We'll get those made fresh for you right away. Would you like to have them to stay or to go?\", 'decision': <GuardDecisionType.allowed: 'allowed'>, 'target_agent': <AgentType.update_order_agent: 'update_order_agent'>, 'order': [{'name': 'Cappuccino', 'quantity': 3, 'per_unit_price': 4.5, 'total_price': 13.5}], 'final_price': 13.5, 'memory_node': False}}\n"
     ]
    }
   ],
   "source": [
    "print(get_bot_response(user_input=\"make it 3\" , user_id=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Backend.utils.util import load_llm\n",
    "from Backend.schemas.state_schema import OrderUpdateState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of India is New Delhi.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 15, 'total_tokens': 24, 'completion_time': 0.015299031, 'prompt_time': 0.01023275, 'queue_time': 0.369110689, 'total_time': 0.025531781}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_0fb809dba3', 'finish_reason': 'stop', 'logprobs': None}, id='run--694f7235-f927-41fd-89ed-536571b7bb4e-0', usage_metadata={'input_tokens': 15, 'output_tokens': 9, 'total_tokens': 24})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is capital of india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama3-8b-8192\")\n",
    "llm = llm.with_structured_output(OrderUpdateState)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderUpdateState(updates=[OrderUpdateItem(name='Latte', set_quantity=None, delta_quantity=1)])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"add one more Latte \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0 , api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of India is **New Delhi**.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []}, id='run--7448b72e-32ae-48d4-afb7-6919509e5bc8-0', usage_metadata={'input_tokens': 6, 'output_tokens': 9, 'total_tokens': 15, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is capital of india\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'langchain_google_genai' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_google_genai\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlangchain_google_genai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'langchain_google_genai' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import langchain_google_genai\n",
    "print(langchain_google_genai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-google-genai\n",
      "Version: 2.1.8\n",
      "Summary: An integration package connecting Google's genai package and LangChain\n",
      "Home-page: https://github.com/langchain-ai/langchain-google\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /opt/anaconda3/envs/ai/lib/python3.10/site-packages\n",
      "Requires: filetype, google-ai-generativelanguage, langchain-core, pydantic\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
